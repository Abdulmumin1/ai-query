---
title: "Agent Chat Methods"
description: "Chat and streaming methods available on the Agent class"
---

The `Agent` class provides built-in chat capabilities. This page documents the chat-related methods.

## Import

```python
from ai_query import Agent
```

## Chat Methods

### chat

```python
async def chat(
    self,
    message: str | list[ContentPart],
    *,
    signal: AbortSignal | None = None,
) -> str
```

Send a message and get an AI response. The message and response are automatically added to the conversation history and persisted.

**Parameters:**

<ParamField path="message" type="str | list[ContentPart]" required>
  The user's message. Can be a string or a list of content parts for multimodal input.
</ParamField>

<ParamField path="signal" type="AbortSignal | None" default="None">
  Optional abort signal for cancellation.
</ParamField>

**Returns:** The AI's response text.

**Example:**

```python
from ai_query import Agent, openai, MemoryStorage

agent = Agent(
    "assistant",
    model=openai("gpt-4o"),
    storage=MemoryStorage()
)

async with agent:
    response = await agent.chat("Hello!")
    print(response)

    # With abort signal
    from ai_query import AbortController
    controller = AbortController()
    response = await agent.chat("Long task", signal=controller.signal)
```

### stream

```python
async def stream(
    self,
    message: str | list[ContentPart],
    *,
    signal: AbortSignal | None = None,
) -> AsyncIterator[str]
```

Stream an AI response chunk by chunk.

**Parameters:**

<ParamField path="message" type="str | list[ContentPart]" required>
  The user's message.
</ParamField>

<ParamField path="signal" type="AbortSignal | None" default="None">
  Optional abort signal for cancellation.
</ParamField>

**Yields:** Response text chunks.

**Example:**

```python
async with agent:
    async for chunk in agent.stream("Tell me a story"):
        print(chunk, end="", flush=True)
```

## Output Property

### output

```python
@property
def output(self) -> AgentOutput
```

Get the current output channel adapter. Use this in hooks to send feedback (status updates, tool execution info) regardless of the transport (WebSocket, SSE, etc.).

Returns `NullOutput` if no output channel is active.

**Example:**

```python
class MyAgent(Agent):
    async def on_step_finish(self, event):
        if event.step.tool_calls:
            for tc in event.step.tool_calls:
                await self.output.send_status(f"Executed: {tc.name}")
```

## Usage Examples

### Basic Chat

```python
from ai_query import Agent, openai, MemoryStorage

agent = Agent(
    "bot-1",
    model=openai("gpt-4o"),
    system="You are a helpful assistant.",
    storage=MemoryStorage()
)

async with agent:
    response = await agent.chat("Hello!")
    print(response)
```

### Streaming Chat

```python
async with agent:
    async for chunk in agent.stream("Tell me a story"):
        print(chunk, end="", flush=True)
```

### With Tools

```python
from ai_query import Agent, openai, MemoryStorage, tool, Field

@tool(description="Get current time")
def get_time() -> str:
    from datetime import datetime
    return datetime.now().isoformat()

agent = Agent(
    "assistant",
    model=openai("gpt-4o"),
    storage=MemoryStorage(),
    tools={"get_time": get_time}
)

async with agent:
    response = await agent.chat("What time is it?")
    print(response)
```

### With Persistent Storage

```python
from ai_query import Agent, SQLiteStorage, openai

agent = Agent(
    "user-123",
    model=openai("gpt-4o"),
    system="You remember our previous conversations.",
    storage=SQLiteStorage("./chat_history.db")
)

# First session
async with agent:
    await agent.chat("My name is Alice")

# Later session - history is preserved
async with agent:
    response = await agent.chat("What's my name?")
    # AI remembers: "Your name is Alice"
```

## WebSocket + SSE Streaming

For real-time apps, use the built-in server with SSE for AI streaming:

```python
from ai_query import Agent, openai, MemoryStorage

class RealtimeBot(Agent):
    def __init__(self):
        super().__init__(
            "realtime-bot",
            model=openai("gpt-4o"),
            system="You are a real-time assistant.",
            storage=MemoryStorage()
        )

    async def on_message(self, connection, message):
        # User messages via WebSocket
        await self.broadcast(f"User: {message}")

        # AI responses streamed
        async for chunk in self.stream(message):
            await connection.send(chunk)

# Start server with WebSocket + SSE
RealtimeBot().serve(port=8080)
# WebSocket: ws://localhost:8080/ws
# SSE: http://localhost:8080/events
```

## Multimodal Input

Send images or other content types:

```python
from ai_query import Agent, openai, MemoryStorage
from ai_query.types import ImagePart, TextPart

agent = Agent("vision-bot", model=openai("gpt-4o"), storage=MemoryStorage())

async with agent:
    response = await agent.chat([
        TextPart(text="What's in this image?"),
        ImagePart(url="https://example.com/image.jpg")
    ])
    print(response)
```
